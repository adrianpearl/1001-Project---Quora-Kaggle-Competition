{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "#from sklearn import cross_validation\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import string, re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00043c2c68e74328c456</th>\n",
       "      <td>What do physicists, mathematicians, computer scientists and philosophers think of David Deutsch's 'Constructor Theory'?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00043d911af1cfbdb5f3</th>\n",
       "      <td>Why are old scriptures from eastern cultures appear lost in the current culture?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000441059c27001eb255</th>\n",
       "      <td>Can I know my I.Q, even if I hate numbers?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00045f3b9fcb27975e26</th>\n",
       "      <td>How can I really make up my mind and get rid of my bad habits like procrastination?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00046512985c0996339e</th>\n",
       "      <td>Was there any relationship between Napoleon and Ali Pasha of Tepelene?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000467723d6f04760035</th>\n",
       "      <td>Where are presynaptic neurons found?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000477ab08d14b6a047d</th>\n",
       "      <td>What ways will a narcissist mother punish her child for going no contact if child goes back to contact with her?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000485e6dd4b149fe051</th>\n",
       "      <td>Can I start freelancing after finishing Udacity's Android basic nanodegree?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000488ff2dbaa802b4d9</th>\n",
       "      <td>What is the reason why we really need Bitcoin?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004a41beea5f02d85ef</th>\n",
       "      <td>What are some good songs for a long journey?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004a7fcb2bf73076489</th>\n",
       "      <td>If blacks support school choice and mandatory sentencing for criminals why don't they vote Republican?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000500e5d543e112707e</th>\n",
       "      <td>What should be added to thrice the rational number -8/9 to get 4/7?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005037036efa7507699</th>\n",
       "      <td>In a four movement symphony, what is typically the form of the second movement?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000525a468b75805135d</th>\n",
       "      <td>What are various stock exchanges?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00052793eaa287aff1e1</th>\n",
       "      <td>I am gay boy and I love my cousin (boy). He is sexy, but I dont know what to do. He is hot, and I want to see his di**. What should I do?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000537213b01fd77b58a</th>\n",
       "      <td>Which races have the smallest penis?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005401e50ab76bb603d</th>\n",
       "      <td>What do the terms Computer Architecture, Microarchitecture and Instruction set Architecture(ISA) mean?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000546d4647809387b91</th>\n",
       "      <td>Are iPhone users psychologically trapped in brand naming and avoiding to complain about very reasonable missing features such as battery...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00056c4d8c51ad92d3d1</th>\n",
       "      <td>How long is one semester in Ontario?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00056d45a1ce63856fc6</th>\n",
       "      <td>Why do females find penises ugly?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000585cb79d4fad61fb0</th>\n",
       "      <td>What are the best ways to add flour to a homemade chili?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005893de9a677ad97c2</th>\n",
       "      <td>What advice do you have for anyone who wishes to accomplish what you have?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059fdb387101ce73d3</th>\n",
       "      <td>How do overcome my extreme fear of insects and bugs?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059fe5d7eea7b17e77</th>\n",
       "      <td>What are some examples of durable work gloves?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005a7bae94f5a721613</th>\n",
       "      <td>2 cars move 6 miles towards each other and then move 8 miles in oppostie direction, what is the distance between them?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005ac1fb4efa2e7f266</th>\n",
       "      <td>What apps do you use the most for work in a given day?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005c5586f8ced327bf2</th>\n",
       "      <td>Can R only take a maximum 100 headers in a dataset? I noticed it will only print the first 100. Is there anyway to change that?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005de07b07a17046e27</th>\n",
       "      <td>How do I marry an American woman for a Green Card? How much do they charge?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005f305899be65ed771</th>\n",
       "      <td>What do companies look for when recruiting expat? Does the nationality of the candidate play a huge role?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005f8861d29aacb4909</th>\n",
       "      <td>Can transcranial magnetic stimulation (TMS) be used to increase cognitive performance?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007a04df07cfffbd57f</th>\n",
       "      <td>Which PHP framework should I choose for a website like OLX?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007ac776554ab510dd3</th>\n",
       "      <td>What do you do as a forensic psychologist?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007adf025f36909b7f5</th>\n",
       "      <td>What can I do to be a better student in school?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007b4c274692e3c64ae</th>\n",
       "      <td>Why does Fortnite lag everytime Iâ€™m fighting someone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007cbd9381647f3b8bf</th>\n",
       "      <td>What are 5 examples of neurotransmitters?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007d02f7daf548678c2</th>\n",
       "      <td>Why are puppies so hyper?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007dfa486fbf6beb840</th>\n",
       "      <td>Is Jhargram, Birbhum a safe place to visit in winter or there is any fear of terrorist or something?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007f9c2973ff3a3d7ec</th>\n",
       "      <td>How can I discover the one thing to change my life instantly?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000809362bf3e0d58a37</th>\n",
       "      <td>Why does my puppy lick his paws?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008304e8c820a60d160</th>\n",
       "      <td>Which is best powerbank for iPhone 7 in India?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00083b24297e4e627776</th>\n",
       "      <td>If an average NFL team obtained a kicker who could consistently make a field goals from 100 yards, would they win the Super Bowl?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00083b5c34d0b450557f</th>\n",
       "      <td>Can I kill myself now?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00084795e25cb5ddc2fc</th>\n",
       "      <td>How can one earn from trading in derivative market?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00084bd3de3fbb2bc124</th>\n",
       "      <td>What are some of the fastest cars in the world?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000861ec8c798f929f3c</th>\n",
       "      <td>When can we know God's timing?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00087b810447c83e508b</th>\n",
       "      <td>What should I learn after HTML&amp;CSS to become a good Front-End developer?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008819c735204821d7d</th>\n",
       "      <td>Why do Lingayats bury their dead rather than burn like the other Hindus?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000892d68b1b103744d8</th>\n",
       "      <td>Is there a therapist who can help me regarding mental health issues?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008d04b7d99d9a4f3c8</th>\n",
       "      <td>What is the remaining useful life for a Honda Step WGN with a mileage of 170000?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008d0a5694cd04caf60</th>\n",
       "      <td>Do you think it's a sign of progress that Singaporeans are adopting British, American, and Australian accents?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008d4b9f4f900554027</th>\n",
       "      <td>I'm the DS PokÃ©mon Diamond game, how do you get to the top of Mt. Coronet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008d70754ce2285a568</th>\n",
       "      <td>How do I create a merchant tracking system?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008d98781f4dc1e3676</th>\n",
       "      <td>What is your life like 5 years from today?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008de6b1e6f9ed1f482</th>\n",
       "      <td>Why do people smoke clove cigarettes if they are very harmful?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008e27768ec30823036</th>\n",
       "      <td>How is Arnab Goswami able to ignore so many defamation cases against him while Kejriwal is being dragged to court everyday on such cases?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008e6b80183d5bb1a29</th>\n",
       "      <td>Which is the best Institute for Big Data Hadoop course in Chandigarh?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00090a521c5dcb091f09</th>\n",
       "      <td>What is the clipboard feature on a computer?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091d5329e4e47d7740</th>\n",
       "      <td>What is the best place to buy maths/physics T-shirts?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091e28b6fe65b3d7d0</th>\n",
       "      <td>Does the people who are rich and still claim reservation have any conscience? Why are they given reservation if are rich?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00092087b5cd4d0e35ec</th>\n",
       "      <td>Are there instances of Medieval Latin text being later \"translated\" into Classical Latin (for the purpose of comparison)?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    question_text  \\\n",
       "qid                                                                                                                                                                 \n",
       "00043c2c68e74328c456                      What do physicists, mathematicians, computer scientists and philosophers think of David Deutsch's 'Constructor Theory'?   \n",
       "00043d911af1cfbdb5f3                                                             Why are old scriptures from eastern cultures appear lost in the current culture?   \n",
       "000441059c27001eb255                                                                                                   Can I know my I.Q, even if I hate numbers?   \n",
       "00045f3b9fcb27975e26                                                          How can I really make up my mind and get rid of my bad habits like procrastination?   \n",
       "00046512985c0996339e                                                                       Was there any relationship between Napoleon and Ali Pasha of Tepelene?   \n",
       "000467723d6f04760035                                                                                                         Where are presynaptic neurons found?   \n",
       "000477ab08d14b6a047d                             What ways will a narcissist mother punish her child for going no contact if child goes back to contact with her?   \n",
       "000485e6dd4b149fe051                                                                  Can I start freelancing after finishing Udacity's Android basic nanodegree?   \n",
       "000488ff2dbaa802b4d9                                                                                               What is the reason why we really need Bitcoin?   \n",
       "0004a41beea5f02d85ef                                                                                                 What are some good songs for a long journey?   \n",
       "0004a7fcb2bf73076489                                       If blacks support school choice and mandatory sentencing for criminals why don't they vote Republican?   \n",
       "000500e5d543e112707e                                                                          What should be added to thrice the rational number -8/9 to get 4/7?   \n",
       "0005037036efa7507699                                                              In a four movement symphony, what is typically the form of the second movement?   \n",
       "000525a468b75805135d                                                                                                            What are various stock exchanges?   \n",
       "00052793eaa287aff1e1    I am gay boy and I love my cousin (boy). He is sexy, but I dont know what to do. He is hot, and I want to see his di**. What should I do?   \n",
       "000537213b01fd77b58a                                                                                                         Which races have the smallest penis?   \n",
       "0005401e50ab76bb603d                                       What do the terms Computer Architecture, Microarchitecture and Instruction set Architecture(ISA) mean?   \n",
       "000546d4647809387b91  Are iPhone users psychologically trapped in brand naming and avoiding to complain about very reasonable missing features such as battery...   \n",
       "00056c4d8c51ad92d3d1                                                                                                         How long is one semester in Ontario?   \n",
       "00056d45a1ce63856fc6                                                                                                            Why do females find penises ugly?   \n",
       "000585cb79d4fad61fb0                                                                                     What are the best ways to add flour to a homemade chili?   \n",
       "0005893de9a677ad97c2                                                                   What advice do you have for anyone who wishes to accomplish what you have?   \n",
       "00059fdb387101ce73d3                                                                                         How do overcome my extreme fear of insects and bugs?   \n",
       "00059fe5d7eea7b17e77                                                                                               What are some examples of durable work gloves?   \n",
       "0005a7bae94f5a721613                       2 cars move 6 miles towards each other and then move 8 miles in oppostie direction, what is the distance between them?   \n",
       "0005ac1fb4efa2e7f266                                                                                       What apps do you use the most for work in a given day?   \n",
       "0005c5586f8ced327bf2              Can R only take a maximum 100 headers in a dataset? I noticed it will only print the first 100. Is there anyway to change that?   \n",
       "0005de07b07a17046e27                                                                  How do I marry an American woman for a Green Card? How much do they charge?   \n",
       "0005f305899be65ed771                                    What do companies look for when recruiting expat? Does the nationality of the candidate play a huge role?   \n",
       "0005f8861d29aacb4909                                                       Can transcranial magnetic stimulation (TMS) be used to increase cognitive performance?   \n",
       "...                                                                                                                                                           ...   \n",
       "0007a04df07cfffbd57f                                                                                  Which PHP framework should I choose for a website like OLX?   \n",
       "0007ac776554ab510dd3                                                                                                   What do you do as a forensic psychologist?   \n",
       "0007adf025f36909b7f5                                                                                              What can I do to be a better student in school?   \n",
       "0007b4c274692e3c64ae                                                                                        Why does Fortnite lag everytime Iâ€™m fighting someone?   \n",
       "0007cbd9381647f3b8bf                                                                                                    What are 5 examples of neurotransmitters?   \n",
       "0007d02f7daf548678c2                                                                                                                    Why are puppies so hyper?   \n",
       "0007dfa486fbf6beb840                                         Is Jhargram, Birbhum a safe place to visit in winter or there is any fear of terrorist or something?   \n",
       "0007f9c2973ff3a3d7ec                                                                                How can I discover the one thing to change my life instantly?   \n",
       "000809362bf3e0d58a37                                                                                                             Why does my puppy lick his paws?   \n",
       "0008304e8c820a60d160                                                                                               Which is best powerbank for iPhone 7 in India?   \n",
       "00083b24297e4e627776            If an average NFL team obtained a kicker who could consistently make a field goals from 100 yards, would they win the Super Bowl?   \n",
       "00083b5c34d0b450557f                                                                                                                       Can I kill myself now?   \n",
       "00084795e25cb5ddc2fc                                                                                          How can one earn from trading in derivative market?   \n",
       "00084bd3de3fbb2bc124                                                                                              What are some of the fastest cars in the world?   \n",
       "000861ec8c798f929f3c                                                                                                               When can we know God's timing?   \n",
       "00087b810447c83e508b                                                                     What should I learn after HTML&CSS to become a good Front-End developer?   \n",
       "0008819c735204821d7d                                                                     Why do Lingayats bury their dead rather than burn like the other Hindus?   \n",
       "000892d68b1b103744d8                                                                         Is there a therapist who can help me regarding mental health issues?   \n",
       "0008d04b7d99d9a4f3c8                                                             What is the remaining useful life for a Honda Step WGN with a mileage of 170000?   \n",
       "0008d0a5694cd04caf60                               Do you think it's a sign of progress that Singaporeans are adopting British, American, and Australian accents?   \n",
       "0008d4b9f4f900554027                                                                   I'm the DS PokÃ©mon Diamond game, how do you get to the top of Mt. Coronet?   \n",
       "0008d70754ce2285a568                                                                                                  How do I create a merchant tracking system?   \n",
       "0008d98781f4dc1e3676                                                                                                   What is your life like 5 years from today?   \n",
       "0008de6b1e6f9ed1f482                                                                               Why do people smoke clove cigarettes if they are very harmful?   \n",
       "0008e27768ec30823036    How is Arnab Goswami able to ignore so many defamation cases against him while Kejriwal is being dragged to court everyday on such cases?   \n",
       "0008e6b80183d5bb1a29                                                                        Which is the best Institute for Big Data Hadoop course in Chandigarh?   \n",
       "00090a521c5dcb091f09                                                                                                 What is the clipboard feature on a computer?   \n",
       "00091d5329e4e47d7740                                                                                        What is the best place to buy maths/physics T-shirts?   \n",
       "00091e28b6fe65b3d7d0                    Does the people who are rich and still claim reservation have any conscience? Why are they given reservation if are rich?   \n",
       "00092087b5cd4d0e35ec                    Are there instances of Medieval Latin text being later \"translated\" into Classical Latin (for the purpose of comparison)?   \n",
       "\n",
       "                      target  \n",
       "qid                           \n",
       "00043c2c68e74328c456       0  \n",
       "00043d911af1cfbdb5f3       0  \n",
       "000441059c27001eb255       0  \n",
       "00045f3b9fcb27975e26       0  \n",
       "00046512985c0996339e       0  \n",
       "000467723d6f04760035       0  \n",
       "000477ab08d14b6a047d       0  \n",
       "000485e6dd4b149fe051       0  \n",
       "000488ff2dbaa802b4d9       0  \n",
       "0004a41beea5f02d85ef       0  \n",
       "0004a7fcb2bf73076489       1  \n",
       "000500e5d543e112707e       0  \n",
       "0005037036efa7507699       0  \n",
       "000525a468b75805135d       0  \n",
       "00052793eaa287aff1e1       1  \n",
       "000537213b01fd77b58a       1  \n",
       "0005401e50ab76bb603d       0  \n",
       "000546d4647809387b91       0  \n",
       "00056c4d8c51ad92d3d1       0  \n",
       "00056d45a1ce63856fc6       1  \n",
       "000585cb79d4fad61fb0       0  \n",
       "0005893de9a677ad97c2       0  \n",
       "00059fdb387101ce73d3       0  \n",
       "00059fe5d7eea7b17e77       0  \n",
       "0005a7bae94f5a721613       0  \n",
       "0005ac1fb4efa2e7f266       0  \n",
       "0005c5586f8ced327bf2       0  \n",
       "0005de07b07a17046e27       1  \n",
       "0005f305899be65ed771       0  \n",
       "0005f8861d29aacb4909       0  \n",
       "...                      ...  \n",
       "0007a04df07cfffbd57f       0  \n",
       "0007ac776554ab510dd3       0  \n",
       "0007adf025f36909b7f5       0  \n",
       "0007b4c274692e3c64ae       0  \n",
       "0007cbd9381647f3b8bf       0  \n",
       "0007d02f7daf548678c2       0  \n",
       "0007dfa486fbf6beb840       0  \n",
       "0007f9c2973ff3a3d7ec       0  \n",
       "000809362bf3e0d58a37       0  \n",
       "0008304e8c820a60d160       0  \n",
       "00083b24297e4e627776       0  \n",
       "00083b5c34d0b450557f       0  \n",
       "00084795e25cb5ddc2fc       0  \n",
       "00084bd3de3fbb2bc124       0  \n",
       "000861ec8c798f929f3c       0  \n",
       "00087b810447c83e508b       0  \n",
       "0008819c735204821d7d       0  \n",
       "000892d68b1b103744d8       0  \n",
       "0008d04b7d99d9a4f3c8       0  \n",
       "0008d0a5694cd04caf60       0  \n",
       "0008d4b9f4f900554027       0  \n",
       "0008d70754ce2285a568       0  \n",
       "0008d98781f4dc1e3676       0  \n",
       "0008de6b1e6f9ed1f482       0  \n",
       "0008e27768ec30823036       0  \n",
       "0008e6b80183d5bb1a29       0  \n",
       "00090a521c5dcb091f09       0  \n",
       "00091d5329e4e47d7740       0  \n",
       "00091e28b6fe65b3d7d0       0  \n",
       "00092087b5cd4d0e35ec       0  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 140)\n",
    "data[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_small = data.sample(100000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop =  stop_words.ENGLISH_STOP_WORDS\n",
    "# stemmer = SnowballStemmer('english')\n",
    "# data_stemmed = data.copy()\n",
    "# data_stemmed['question_text'] = [' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "#           for text in data_stemmed.question_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep_training(dataset, training_split, test_split):\n",
    "    train, test = train_test_split(dataset, train_size = training_split, test_size = test_split)\n",
    "    train_downsampled = downsample(train)\n",
    "    \n",
    "    X_train = train_downsampled['question_text']\n",
    "    Y_train = train_downsampled['target']\n",
    "    X_test = test['question_text']\n",
    "    Y_test = test['target']\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def data_prep_training_nodownsample(dataset, training_split, test_split):\n",
    "    train, test = train_test_split(dataset, train_size = training_split, test_size = test_split)\n",
    "    #train_downsampled = downsample(train)\n",
    "    \n",
    "    X_train = train['question_text']\n",
    "    Y_train = train['target']\n",
    "    X_test = test['question_text']\n",
    "    Y_test = test['target']\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def data_prep(dataset, training_split, test_split):\n",
    "    X = dataset['question_text']\n",
    "    Y = dataset['target']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = training_split, test_size = test_split)\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "\n",
    "def model_vectorize(data_used, vectorizer_type, binary_type, ngram, stop_word, model_type):\n",
    "    X_train = data_used[0]\n",
    "    X_test = data_used[1]\n",
    "    Y_train = data_used[2]\n",
    "    Y_test = data_used[3]\n",
    "    \n",
    "    vectorizer = vectorizer_type(binary = binary_type, stop_words = stop_word, ngram_range=ngram)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_vectorized = vectorizer.transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    model = model_type\n",
    "    model.fit(X_train_vectorized, Y_train)\n",
    "    \n",
    "    scores = model.predict_proba(X_test_vectorized)[:,1]\n",
    "    precision, recall, thresholds = precision_recall_curve(Y_test, scores)\n",
    "    precision, recall = precision[:-1], recall[:-1]\n",
    "    fscores = 2*np.divide(np.multiply(precision, recall), np.add(precision, recall))\n",
    "    max_fscore = np.nanmax(fscores)\n",
    "    ind_max = fscores.argmax() #not sure this is working\n",
    "    threshold_max = thresholds[ind_max]\n",
    "    \n",
    "    return(max_fscore, threshold_max)\n",
    "\n",
    "\n",
    "def downsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.target==0]\n",
    "    df_minority = df[df.target==1]\n",
    "\n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=df_minority.shape[0],     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "    \n",
    "    return (df_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# downsample_data_regular = data_prep(downsample(data), .9, .1)\n",
    "# downsample_data_stemmed = data_prep(downsample(data_stemmed), .9,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test = model_vectorize(downsample_data_regular, CountVectorizer, True, (1,2), None, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fscore</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897366</td>\n",
       "      <td>0.431035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fscore  threshold\n",
       "0  0.897366   0.431035"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fscores = []\n",
    "# thresholds = []\n",
    "# fscores.append(test[0])\n",
    "# thresholds.append(test[1])\n",
    "# metrics = pd.DataFrame({'fscore': fscores, 'threshold': thresholds})\n",
    "# metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test2 = model_vectorize(downsample_data_regular, CountVectorizer, True, (1,2), None, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fscore</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.872824</td>\n",
       "      <td>0.041563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fscore  threshold\n",
       "0  0.872824   0.041563"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fscores = []\n",
    "# thresholds = []\n",
    "# fscores.append(test2[0])\n",
    "# thresholds.append(test2[1])\n",
    "# metrics = pd.DataFrame({'fscore': fscores, 'threshold': thresholds})\n",
    "# metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test3 = model_vectorize(downsample_data_stemmed, CountVectorizer, True, (1,2), None, BernoulliNB())\n",
    "# test4 = model_vectorize(downsample_data_regular, CountVectorizer, True, (1,3), None, BernoulliNB())\n",
    "# test5 = model_vectorize(downsample_data_regular, CountVectorizer, True, (1,3), stop, BernoulliNB())\n",
    "# test6 = model_vectorize(downsample_data_regular, TfidfVectorizer, False, (1,2), None, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fscore</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897366</td>\n",
       "      <td>0.431035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.878318</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.872824</td>\n",
       "      <td>0.041563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.872824</td>\n",
       "      <td>0.041563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872032</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.871758</td>\n",
       "      <td>0.037112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fscore  threshold\n",
       "0  0.897366   0.431035\n",
       "4  0.878318   0.004500\n",
       "1  0.872824   0.041563\n",
       "5  0.872824   0.041563\n",
       "3  0.872032   0.000451\n",
       "2  0.871758   0.037112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fscores = []\n",
    "# thresholds = []\n",
    "# for i in [test, test2, test3, test4, test5, test6]:\n",
    "#     fscores.append(i[0])\n",
    "#     thresholds.append(i[1])\n",
    "# metrics = pd.DataFrame({'fscore': fscores, 'threshold': thresholds})\n",
    "# metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(downsample_data_regular)\n",
    "# #data.target.value_counts()\n",
    "# #len(downsample_data_regular[0]), len(downsample_data_regular[1]), len(downsample_data_regular[2]), len(downsample_data_regular[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #create models\n",
    "# dataset = [downsample_data_regular, downsample_data_stemmed]\n",
    "# vectorizer_type = [CountVectorizer, TfidfVectorizer]\n",
    "# binary_type = [True, False]\n",
    "# ngram = [(1,2), (1,3), (1,4)]\n",
    "# stop_word = [None, stop]\n",
    "# model_type = [LogisticRegression(), BernoulliNB()]\n",
    "\n",
    "# model_initialize = []\n",
    "# for h in dataset:\n",
    "#     for i in vectorizer_type:\n",
    "#         for j in binary_type:\n",
    "#             for k in ngram:\n",
    "#                 for l in model_type:\n",
    "#                     for m in stop_word:\n",
    "#                         model_initialize.append(model_vectorize(h, i, j, k, m, l))\n",
    "                    \n",
    "# #create labels\n",
    "# dataset_label = ['Downsample Full Regular Data', 'Downsample Full Stemmed Data']\n",
    "# vectorizer_type_label = ['CountV', 'TFIDV']\n",
    "# binary_type_label = ['T', 'F']\n",
    "# ngram_label = [(1,2), (1,3), (1,4)]\n",
    "# stop_word_label = ['None', 'english']\n",
    "# model_type_label = ['LR', 'NB']\n",
    "\n",
    "# labels = []\n",
    "# for h in dataset_label:    \n",
    "#     for i in vectorizer_type_label:\n",
    "#         for j in binary_type_label:\n",
    "#             for k in ngram_label:\n",
    "#                 for l in model_type_label:\n",
    "#                     for m in stop_word_label:\n",
    "#                         label = '%s %s %s %s %s %s' %(h, i, j, k, m, l)\n",
    "#                         labels.append(label)\n",
    "                        \n",
    "# fscores = []\n",
    "# thresholds = []\n",
    "# for i in model_initialize:\n",
    "#     fscores.append(i[0])\n",
    "#     thresholds.append(i[1])\n",
    "# metrics = pd.DataFrame({'label': labels,'fscore': fscores, 'threshold': thresholds})\n",
    "# metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(train_data, test_data, vectorizer_type, binary_type, ngram, stop_word, model_type, max_t):\n",
    "    X_train = train_data['question_text']\n",
    "    Y_train = train_data['target']\n",
    "    X_test = test_data['question_text']\n",
    "    \n",
    "    \n",
    "    vectorizer = vectorizer_type(binary = binary_type, stop_words = stop_word, ngram_range=ngram)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_vectorized = vectorizer.transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    model = model_type\n",
    "    model.fit(X_train_vectorized, Y_train)\n",
    "    \n",
    "    predictions = model.predict_proba(X_test_vectorized)[:,1]\n",
    "    max_threshold = max_t\n",
    "    targets = []\n",
    "    for i in predictions:\n",
    "        if i<= max_threshold:\n",
    "            targets.append(0)\n",
    "        else:\n",
    "            targets.append(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downsampled = downsample(train_data)\n",
    "test = run_model(downsampled, test_data, CountVectorizer, True, (1,2), None, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "def _get_groupings(groupings_dict):\n",
    "    grouping_re = re.compile('(%s)' % '|'.join(groupings_dict.keys()))\n",
    "    return groupings_dict, grouping_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'wouldnt':'would not',\n",
    "                'isnt':'is not',\n",
    "                'wouldnt':'would not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'neighbour': 'neighbor',\n",
    "                'humour': 'humor',\n",
    "                'apologise': 'apologize',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'recognise': 'recognize',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'travelled': 'traveled',\n",
    "                'offence': 'offense',\n",
    "                'licence': 'license',\n",
    "                'labour':'labor',\n",
    "                'behaviour': 'behavior',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'criticise':'criticize',\n",
    "                }\n",
    "\n",
    "groupings_dict = {'instagram': 'social_media',\n",
    "                  'whatsapp': 'social_media',\n",
    "                  'snapchat': 'social_media',\n",
    "                  'facebook': 'social_media',\n",
    "                  'wechat': 'social_media',\n",
    "                  'groupme': 'social_media',\n",
    "                  'iphone': 'cellphone',\n",
    "                  'samsung galaxy': 'cellphone',\n",
    "                  'google pixel': 'cellphone',\n",
    "                  'president donald trump': 'political_person',\n",
    "                  'donald trump': 'political_person',\n",
    "                  'president trump': 'political_person',\n",
    "                  'trump': 'political_person',\n",
    "                  'president barack obama': 'political_person',\n",
    "                  'president obama': 'political_person',\n",
    "                  'barack obama': 'political_person',\n",
    "                  'obama': 'political_person',\n",
    "                  'george w bush': 'political_person',\n",
    "                  'george trump': 'political_person',\n",
    "                  'president bush': 'political_person',\n",
    "                  'president trump': 'political_person',\n",
    "                  'jfk': 'political_person',\n",
    "                  'president nixon': 'political_person',\n",
    "                  'nixon': 'political_person',\n",
    "                  'democrats': 'political_party',\n",
    "                  'democrat': 'political_party',\n",
    "                  'republicans': 'political_party',\n",
    "                  'democratic': 'political_party',\n",
    "                  'republican': 'political_party',\n",
    "                  'president trump': 'political_person',\n",
    "                  'homosexual': 'lgbtq',\n",
    "                  'heterosexual': 'lgbtq',\n",
    "                  'homosexual': 'lgbtq', \n",
    "                  'bisexual': 'lgbtq', \n",
    "                  'transgender': 'lgbtq', \n",
    "                  'pansexual': 'lgbtq', \n",
    "                  'gen x': 'generation',\n",
    "                  'gen y': 'generation',\n",
    "                  'gen z': 'generation',\n",
    "                  'millennials': 'generation',\n",
    "                  'millennial': 'generation',\n",
    "                  'milennials': 'generation',\n",
    "                  'milennial': 'generation',\n",
    "                  'milenials': 'generation',\n",
    "                  'milenial': 'generation',\n",
    "                  \n",
    "    \n",
    "}\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "groupings, groupings_re = _get_groupings(groupings_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def replace_typical_groupings(text):\n",
    "    def replace(match):\n",
    "        return groupings[match.group(0)]\n",
    "\n",
    "    return groupings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    # Remove puncuation\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"'m\", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    \n",
    "    text = re.sub('[0-9]{5,}', '#####', text)\n",
    "    text = re.sub('[0-9]{4}', '####', text)\n",
    "    text = re.sub('[0-9]{3}', '###', text)\n",
    "    text = re.sub('[0-9]{2}', '##', text)\n",
    "    text = re.sub('[0-9]{1}', '#', text)\n",
    "    \n",
    "    #Replace typical misspells\n",
    "    text = replace_typical_misspell(text)\n",
    "    \n",
    "    #Replace typical groupings\n",
    "    text = replace_typical_groupings(text)\n",
    "    \n",
    "#     #Stem words\n",
    "#     stemmer = SnowballStemmer('english')\n",
    "#     text = ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/train.csv', index_col = 0)\n",
    "train_data['question_text'] = train_data[\"question_text\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/test.csv', index_col = 0)\n",
    "test_data['question_text'] = test_data[\"question_text\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_downsample = data_prep_training(train_data, .9, .1)\n",
    "data_nodownsample = data_prep_training_nodownsample(train_data, .9, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_withgroupings = pd.read_csv('../input/train.csv', index_col = 0)\n",
    "train_data_withgroupings['question_text'] = train_data_withgroupings[\"question_text\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.63941672885929157, 0.16270017933393438)\n"
     ]
    }
   ],
   "source": [
    "data_withgroupings_nodownsample = data_prep_training_nodownsample(train_data_withgroupings, .9, .1)\n",
    "data_withgroupings_nodownsample_results = model_vectorize(data_withgroupings_nodownsample, CountVectorizer, True, (1,4), None, LogisticRegression())\n",
    "print(data_withgroupings_nodownsample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_withgroupings_nodownsample_nostem_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-287ac5560800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_withgroupings_nodownsample_nostem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_training_nodownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_withgroupings_nostem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_withgroupings_nodownsample_no_stem_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_withgroupings_nodownsample_nostem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_withgroupings_nodownsample_nostem_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_withgroupings_nodownsample_nostem_results' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_withgroupings_nostem = pd.read_csv('../input/train.csv', index_col = 0)\n",
    "train_data_withgroupings_nostem['question_text'] = train_data_withgroupings_nostem[\"question_text\"].apply(lambda x: clean(x))\n",
    "data_withgroupings_nodownsample_nostem = data_prep_training_nodownsample(train_data_withgroupings_nostem, .9, .1)\n",
    "data_withgroupings_nodownsample_no_stem_results = model_vectorize(data_withgroupings_nodownsample_nostem, CountVectorizer, True, (1,4), None, LogisticRegression())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.64254803531244598, 0.15558298283311578)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_withgroupings_nodownsample_no_stem_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fscore</th>\n",
       "      <th>label</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.646017</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 3) None LR</td>\n",
       "      <td>1.837884e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.645811</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 4) None LR</td>\n",
       "      <td>1.550502e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.644574</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 4) None LR</td>\n",
       "      <td>1.536640e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.643998</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 3) None LR</td>\n",
       "      <td>1.756927e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.639218</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 4) None LR</td>\n",
       "      <td>1.713877e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.638754</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 3) None LR</td>\n",
       "      <td>1.909556e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.638194</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 4) None LR</td>\n",
       "      <td>1.515478e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.637048</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 2) None LR</td>\n",
       "      <td>2.256010e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.636419</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 3) None LR</td>\n",
       "      <td>1.530486e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.634470</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 2) None LR</td>\n",
       "      <td>2.001840e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.629206</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 2) None LR</td>\n",
       "      <td>2.194776e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.628135</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV F (1, 2) None LR</td>\n",
       "      <td>2.763227e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.627185</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 2) None LR</td>\n",
       "      <td>2.220652e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.626509</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV T (1, 2) None LR</td>\n",
       "      <td>2.624675e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.625726</td>\n",
       "      <td>No downsample Cleaned TFIDV T (1, 2) None LR</td>\n",
       "      <td>2.712564e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.625244</td>\n",
       "      <td>No downsample Cleaned TFIDV F (1, 2) None LR</td>\n",
       "      <td>2.998903e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.622205</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 3) english LR</td>\n",
       "      <td>1.690171e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.622097</td>\n",
       "      <td>No downsample Cleaned TFIDV F (1, 3) None LR</td>\n",
       "      <td>3.209115e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.621582</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 3) english LR</td>\n",
       "      <td>1.732580e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.621296</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 4) english LR</td>\n",
       "      <td>1.650198e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.621234</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 3) english LR</td>\n",
       "      <td>1.684523e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.621055</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 4) english LR</td>\n",
       "      <td>1.580627e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.620805</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 2) english LR</td>\n",
       "      <td>2.006896e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.620778</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 4) english LR</td>\n",
       "      <td>1.557344e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.620576</td>\n",
       "      <td>No downsample Cleaned TFIDV T (1, 3) None LR</td>\n",
       "      <td>9.997943e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.620167</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 3) english LR</td>\n",
       "      <td>1.877862e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.620111</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 4) english LR</td>\n",
       "      <td>1.441428e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.619573</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 2) english LR</td>\n",
       "      <td>1.915848e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.618826</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV F (1, 3) None LR</td>\n",
       "      <td>3.749961e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.618176</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV T (1, 3) None LR</td>\n",
       "      <td>3.344668e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.510114</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 2) None NB</td>\n",
       "      <td>4.478206e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.510114</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 2) None NB</td>\n",
       "      <td>4.478206e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.504608</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 4) english NB</td>\n",
       "      <td>1.446111e-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.504608</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 4) english NB</td>\n",
       "      <td>1.446111e-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.504608</td>\n",
       "      <td>No downsample Cleaned TFIDV T (1, 4) english NB</td>\n",
       "      <td>1.446111e-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.504608</td>\n",
       "      <td>No downsample Cleaned TFIDV F (1, 4) english NB</td>\n",
       "      <td>1.446111e-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.494362</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 3) english NB</td>\n",
       "      <td>1.078388e-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.494362</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV F (1, 3) english NB</td>\n",
       "      <td>1.078388e-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.494362</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 3) english NB</td>\n",
       "      <td>1.078388e-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.494362</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV T (1, 3) english NB</td>\n",
       "      <td>1.078388e-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.491396</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 3) None NB</td>\n",
       "      <td>6.127749e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.491396</td>\n",
       "      <td>No downsample Cleaned TFIDV T (1, 3) None NB</td>\n",
       "      <td>6.127749e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.491396</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 3) None NB</td>\n",
       "      <td>6.127749e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.491396</td>\n",
       "      <td>No downsample Cleaned TFIDV F (1, 3) None NB</td>\n",
       "      <td>6.127749e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.487919</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV T (1, 4) english NB</td>\n",
       "      <td>4.568453e-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.487919</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV F (1, 4) english NB</td>\n",
       "      <td>4.568453e-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.487919</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 4) english NB</td>\n",
       "      <td>4.568453e-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.487919</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 4) english NB</td>\n",
       "      <td>4.568453e-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.485268</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV T (1, 3) None NB</td>\n",
       "      <td>2.053243e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.485268</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 3) None NB</td>\n",
       "      <td>2.053243e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.485268</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV F (1, 3) None NB</td>\n",
       "      <td>2.053243e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.485268</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 3) None NB</td>\n",
       "      <td>2.053243e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.477131</td>\n",
       "      <td>No downsample Cleaned TFIDV F (1, 4) None NB</td>\n",
       "      <td>6.641331e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.477131</td>\n",
       "      <td>No downsample Cleaned TFIDV T (1, 4) None NB</td>\n",
       "      <td>6.641331e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.477131</td>\n",
       "      <td>No downsample Cleaned CountV F (1, 4) None NB</td>\n",
       "      <td>6.641331e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.477131</td>\n",
       "      <td>No downsample Cleaned CountV T (1, 4) None NB</td>\n",
       "      <td>6.641331e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.473355</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV F (1, 4) None NB</td>\n",
       "      <td>2.062535e-95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.473355</td>\n",
       "      <td>No Downsample Cleaned No Stem CountV T (1, 4) None NB</td>\n",
       "      <td>2.062535e-95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.473355</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV F (1, 4) None NB</td>\n",
       "      <td>2.062535e-95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.473355</td>\n",
       "      <td>No Downsample Cleaned No Stem TFIDV T (1, 4) None NB</td>\n",
       "      <td>2.062535e-95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fscore                                                     label  \\\n",
       "52  0.646017     No Downsample Cleaned No Stem CountV T (1, 3) None LR   \n",
       "56  0.645811     No Downsample Cleaned No Stem CountV T (1, 4) None LR   \n",
       "68  0.644574     No Downsample Cleaned No Stem CountV F (1, 4) None LR   \n",
       "64  0.643998     No Downsample Cleaned No Stem CountV F (1, 3) None LR   \n",
       "8   0.639218             No downsample Cleaned CountV T (1, 4) None LR   \n",
       "4   0.638754             No downsample Cleaned CountV T (1, 3) None LR   \n",
       "20  0.638194             No downsample Cleaned CountV F (1, 4) None LR   \n",
       "48  0.637048     No Downsample Cleaned No Stem CountV T (1, 2) None LR   \n",
       "16  0.636419             No downsample Cleaned CountV F (1, 3) None LR   \n",
       "60  0.634470     No Downsample Cleaned No Stem CountV F (1, 2) None LR   \n",
       "0   0.629206             No downsample Cleaned CountV T (1, 2) None LR   \n",
       "84  0.628135      No Downsample Cleaned No Stem TFIDV F (1, 2) None LR   \n",
       "12  0.627185             No downsample Cleaned CountV F (1, 2) None LR   \n",
       "72  0.626509      No Downsample Cleaned No Stem TFIDV T (1, 2) None LR   \n",
       "24  0.625726              No downsample Cleaned TFIDV T (1, 2) None LR   \n",
       "36  0.625244              No downsample Cleaned TFIDV F (1, 2) None LR   \n",
       "17  0.622205          No downsample Cleaned CountV F (1, 3) english LR   \n",
       "40  0.622097              No downsample Cleaned TFIDV F (1, 3) None LR   \n",
       "5   0.621582          No downsample Cleaned CountV T (1, 3) english LR   \n",
       "9   0.621296          No downsample Cleaned CountV T (1, 4) english LR   \n",
       "53  0.621234  No Downsample Cleaned No Stem CountV T (1, 3) english LR   \n",
       "69  0.621055  No Downsample Cleaned No Stem CountV F (1, 4) english LR   \n",
       "49  0.620805  No Downsample Cleaned No Stem CountV T (1, 2) english LR   \n",
       "57  0.620778  No Downsample Cleaned No Stem CountV T (1, 4) english LR   \n",
       "28  0.620576              No downsample Cleaned TFIDV T (1, 3) None LR   \n",
       "65  0.620167  No Downsample Cleaned No Stem CountV F (1, 3) english LR   \n",
       "21  0.620111          No downsample Cleaned CountV F (1, 4) english LR   \n",
       "13  0.619573          No downsample Cleaned CountV F (1, 2) english LR   \n",
       "88  0.618826      No Downsample Cleaned No Stem TFIDV F (1, 3) None LR   \n",
       "76  0.618176      No Downsample Cleaned No Stem TFIDV T (1, 3) None LR   \n",
       "..       ...                                                       ...   \n",
       "62  0.510114     No Downsample Cleaned No Stem CountV F (1, 2) None NB   \n",
       "50  0.510114     No Downsample Cleaned No Stem CountV T (1, 2) None NB   \n",
       "11  0.504608          No downsample Cleaned CountV T (1, 4) english NB   \n",
       "23  0.504608          No downsample Cleaned CountV F (1, 4) english NB   \n",
       "35  0.504608           No downsample Cleaned TFIDV T (1, 4) english NB   \n",
       "47  0.504608           No downsample Cleaned TFIDV F (1, 4) english NB   \n",
       "67  0.494362  No Downsample Cleaned No Stem CountV F (1, 3) english NB   \n",
       "91  0.494362   No Downsample Cleaned No Stem TFIDV F (1, 3) english NB   \n",
       "55  0.494362  No Downsample Cleaned No Stem CountV T (1, 3) english NB   \n",
       "79  0.494362   No Downsample Cleaned No Stem TFIDV T (1, 3) english NB   \n",
       "18  0.491396             No downsample Cleaned CountV F (1, 3) None NB   \n",
       "30  0.491396              No downsample Cleaned TFIDV T (1, 3) None NB   \n",
       "6   0.491396             No downsample Cleaned CountV T (1, 3) None NB   \n",
       "42  0.491396              No downsample Cleaned TFIDV F (1, 3) None NB   \n",
       "83  0.487919   No Downsample Cleaned No Stem TFIDV T (1, 4) english NB   \n",
       "95  0.487919   No Downsample Cleaned No Stem TFIDV F (1, 4) english NB   \n",
       "59  0.487919  No Downsample Cleaned No Stem CountV T (1, 4) english NB   \n",
       "71  0.487919  No Downsample Cleaned No Stem CountV F (1, 4) english NB   \n",
       "78  0.485268      No Downsample Cleaned No Stem TFIDV T (1, 3) None NB   \n",
       "54  0.485268     No Downsample Cleaned No Stem CountV T (1, 3) None NB   \n",
       "90  0.485268      No Downsample Cleaned No Stem TFIDV F (1, 3) None NB   \n",
       "66  0.485268     No Downsample Cleaned No Stem CountV F (1, 3) None NB   \n",
       "46  0.477131              No downsample Cleaned TFIDV F (1, 4) None NB   \n",
       "34  0.477131              No downsample Cleaned TFIDV T (1, 4) None NB   \n",
       "22  0.477131             No downsample Cleaned CountV F (1, 4) None NB   \n",
       "10  0.477131             No downsample Cleaned CountV T (1, 4) None NB   \n",
       "70  0.473355     No Downsample Cleaned No Stem CountV F (1, 4) None NB   \n",
       "58  0.473355     No Downsample Cleaned No Stem CountV T (1, 4) None NB   \n",
       "94  0.473355      No Downsample Cleaned No Stem TFIDV F (1, 4) None NB   \n",
       "82  0.473355      No Downsample Cleaned No Stem TFIDV T (1, 4) None NB   \n",
       "\n",
       "       threshold  \n",
       "52  1.837884e-01  \n",
       "56  1.550502e-01  \n",
       "68  1.536640e-01  \n",
       "64  1.756927e-01  \n",
       "8   1.713877e-01  \n",
       "4   1.909556e-01  \n",
       "20  1.515478e-01  \n",
       "48  2.256010e-01  \n",
       "16  1.530486e-01  \n",
       "60  2.001840e-01  \n",
       "0   2.194776e-01  \n",
       "84  2.763227e-01  \n",
       "12  2.220652e-01  \n",
       "72  2.624675e-01  \n",
       "24  2.712564e-01  \n",
       "36  2.998903e-01  \n",
       "17  1.690171e-01  \n",
       "40  3.209115e-01  \n",
       "5   1.732580e-01  \n",
       "9   1.650198e-01  \n",
       "53  1.684523e-01  \n",
       "69  1.580627e-01  \n",
       "49  2.006896e-01  \n",
       "57  1.557344e-01  \n",
       "28  9.997943e-01  \n",
       "65  1.877862e-01  \n",
       "21  1.441428e-01  \n",
       "13  1.915848e-01  \n",
       "88  3.749961e-01  \n",
       "76  3.344668e-01  \n",
       "..           ...  \n",
       "62  4.478206e-12  \n",
       "50  4.478206e-12  \n",
       "11  1.446111e-59  \n",
       "23  1.446111e-59  \n",
       "35  1.446111e-59  \n",
       "47  1.446111e-59  \n",
       "67  1.078388e-41  \n",
       "91  1.078388e-41  \n",
       "55  1.078388e-41  \n",
       "79  1.078388e-41  \n",
       "18  6.127749e-44  \n",
       "30  6.127749e-44  \n",
       "6   6.127749e-44  \n",
       "42  6.127749e-44  \n",
       "83  4.568453e-62  \n",
       "95  4.568453e-62  \n",
       "59  4.568453e-62  \n",
       "71  4.568453e-62  \n",
       "78  2.053243e-48  \n",
       "54  2.053243e-48  \n",
       "90  2.053243e-48  \n",
       "66  2.053243e-48  \n",
       "46  6.641331e-91  \n",
       "34  6.641331e-91  \n",
       "22  6.641331e-91  \n",
       "10  6.641331e-91  \n",
       "70  2.062535e-95  \n",
       "58  2.062535e-95  \n",
       "94  2.062535e-95  \n",
       "82  2.062535e-95  \n",
       "\n",
       "[96 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create models\n",
    "dataset = [data_prep_training_nodownsample(train_data_withgroupings, .9, .1), \n",
    "           data_prep_training_nodownsample(train_data_withgroupings_nostem, .9, .1)]\n",
    "vectorizer_type = [CountVectorizer, TfidfVectorizer]\n",
    "binary_type = [True, False]\n",
    "ngram = [(1,2), (1,3), (1,4)]\n",
    "stop_word = [None, stop]\n",
    "model_type = [LogisticRegression(), BernoulliNB()]\n",
    "\n",
    "model_initialize = []\n",
    "for h in dataset:\n",
    "    for i in vectorizer_type:\n",
    "        for j in binary_type:\n",
    "            for k in ngram:\n",
    "                for l in model_type:\n",
    "                    for m in stop_word:\n",
    "                        model_initialize.append(model_vectorize(h, i, j, k, m, l))\n",
    "                    \n",
    "#create labels\n",
    "dataset_label = ['No downsample Cleaned', 'No Downsample Cleaned No Stem']\n",
    "vectorizer_type_label = ['CountV', 'TFIDV']\n",
    "binary_type_label = ['T', 'F']\n",
    "ngram_label = [(1,2), (1,3), (1,4)]\n",
    "stop_word_label = ['None', 'english']\n",
    "model_type_label = ['LR', 'NB']\n",
    "\n",
    "labels = []\n",
    "for h in dataset_label:    \n",
    "    for i in vectorizer_type_label:\n",
    "        for j in binary_type_label:\n",
    "            for k in ngram_label:\n",
    "                for l in model_type_label:\n",
    "                    for m in stop_word_label:\n",
    "                        label = '%s %s %s %s %s %s' %(h, i, j, k, m, l)\n",
    "                        labels.append(label)\n",
    "                        \n",
    "fscores = []\n",
    "thresholds = []\n",
    "for i in model_initialize:\n",
    "    fscores.append(i[0])\n",
    "    thresholds.append(i[1])\n",
    "metrics = pd.DataFrame({'label': labels,'fscore': fscores, 'threshold': thresholds})\n",
    "metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fscore</th>\n",
       "      <th>label</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.651581</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 4) None LR</td>\n",
       "      <td>1.636935e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.650032</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 4) None LR</td>\n",
       "      <td>1.679055e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.649858</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 3) None LR</td>\n",
       "      <td>1.787951e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.648345</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 3) None LR</td>\n",
       "      <td>1.764204e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.644638</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 2) None LR</td>\n",
       "      <td>2.395410e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.641428</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 2) None LR</td>\n",
       "      <td>2.359062e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.640102</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 2) None LR</td>\n",
       "      <td>2.595899e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.638673</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 2) None LR</td>\n",
       "      <td>2.855823e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.633056</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 3) english LR</td>\n",
       "      <td>1.777751e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.632324</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 2) english LR</td>\n",
       "      <td>2.184303e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.632157</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 3) None LR</td>\n",
       "      <td>9.998686e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.632003</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 3) english LR</td>\n",
       "      <td>2.192545e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.631847</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 4) english LR</td>\n",
       "      <td>1.596882e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.630763</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 3) None LR</td>\n",
       "      <td>9.998404e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.630649</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 4) english LR</td>\n",
       "      <td>1.933252e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.628897</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 2) english LR</td>\n",
       "      <td>1.985770e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.623099</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 4) None LR</td>\n",
       "      <td>9.999696e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.618892</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 4) None LR</td>\n",
       "      <td>9.999574e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.617105</td>\n",
       "      <td>Downsample Cleaned CountV T (1, 3) None LR</td>\n",
       "      <td>8.821356e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.616211</td>\n",
       "      <td>Downsample Cleaned CountV T (1, 4) None LR</td>\n",
       "      <td>8.861428e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.615883</td>\n",
       "      <td>Downsample Cleaned CountV F (1, 3) None LR</td>\n",
       "      <td>8.630417e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615796</td>\n",
       "      <td>Downsample Cleaned CountV T (1, 2) None LR</td>\n",
       "      <td>8.803717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.615759</td>\n",
       "      <td>Downsample Cleaned CountV F (1, 4) None LR</td>\n",
       "      <td>8.866049e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.613679</td>\n",
       "      <td>Downsample Cleaned CountV F (1, 2) None LR</td>\n",
       "      <td>8.589295e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.610002</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 2) english LR</td>\n",
       "      <td>3.149623e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.608004</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 2) english LR</td>\n",
       "      <td>3.061602e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.602546</td>\n",
       "      <td>Downsample Cleaned CountV T (1, 2) english LR</td>\n",
       "      <td>8.263129e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.601552</td>\n",
       "      <td>Downsample Cleaned CountV F (1, 2) english LR</td>\n",
       "      <td>8.251703e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.601429</td>\n",
       "      <td>Downsample Cleaned CountV T (1, 3) english LR</td>\n",
       "      <td>8.405311e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.600690</td>\n",
       "      <td>Downsample Cleaned CountV F (1, 3) english LR</td>\n",
       "      <td>8.150905e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.540632</td>\n",
       "      <td>Downsample Cleaned CountV T (1, 4) None NB</td>\n",
       "      <td>8.400820e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.540632</td>\n",
       "      <td>Downsample Cleaned CountV F (1, 4) None NB</td>\n",
       "      <td>8.400820e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.540632</td>\n",
       "      <td>Downsample Cleaned TFIDV F (1, 4) None NB</td>\n",
       "      <td>8.400820e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.540184</td>\n",
       "      <td>Downsample Cleaned TFIDV T (1, 3) english LR</td>\n",
       "      <td>9.999998e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.537733</td>\n",
       "      <td>Downsample Cleaned TFIDV F (1, 4) english LR</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.531954</td>\n",
       "      <td>Downsample Cleaned TFIDV T (1, 4) english LR</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.530934</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 2) english NB</td>\n",
       "      <td>2.652246e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.530934</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 2) english NB</td>\n",
       "      <td>2.652246e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.530934</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 2) english NB</td>\n",
       "      <td>2.652246e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.530934</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 2) english NB</td>\n",
       "      <td>2.652246e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.521683</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 3) english NB</td>\n",
       "      <td>3.098366e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.521683</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 3) english NB</td>\n",
       "      <td>3.098366e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.521683</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 3) english NB</td>\n",
       "      <td>3.098366e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.521683</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 3) english NB</td>\n",
       "      <td>3.098366e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.517715</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 2) None NB</td>\n",
       "      <td>6.819512e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.517715</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 2) None NB</td>\n",
       "      <td>6.819512e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.517715</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 2) None NB</td>\n",
       "      <td>6.819512e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.517715</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 2) None NB</td>\n",
       "      <td>6.819512e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.513580</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 4) english NB</td>\n",
       "      <td>2.362725e-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.513580</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 4) english NB</td>\n",
       "      <td>2.362725e-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.513580</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 4) english NB</td>\n",
       "      <td>2.362725e-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.513580</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 4) english NB</td>\n",
       "      <td>2.362725e-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.494081</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 3) None NB</td>\n",
       "      <td>2.580253e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.494081</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 3) None NB</td>\n",
       "      <td>2.580253e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.494081</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 3) None NB</td>\n",
       "      <td>2.580253e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.494081</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 3) None NB</td>\n",
       "      <td>2.580253e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.479763</td>\n",
       "      <td>No Downsample Cleaned TFIDV T (1, 4) None NB</td>\n",
       "      <td>2.058903e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.479763</td>\n",
       "      <td>No Downsample Cleaned CountV F (1, 4) None NB</td>\n",
       "      <td>2.058903e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.479763</td>\n",
       "      <td>No Downsample Cleaned CountV T (1, 4) None NB</td>\n",
       "      <td>2.058903e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.479763</td>\n",
       "      <td>No Downsample Cleaned TFIDV F (1, 4) None NB</td>\n",
       "      <td>2.058903e-91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fscore                                             label     threshold\n",
       "56  0.651581     No Downsample Cleaned CountV T (1, 4) None LR  1.636935e-01\n",
       "68  0.650032     No Downsample Cleaned CountV F (1, 4) None LR  1.679055e-01\n",
       "52  0.649858     No Downsample Cleaned CountV T (1, 3) None LR  1.787951e-01\n",
       "64  0.648345     No Downsample Cleaned CountV F (1, 3) None LR  1.764204e-01\n",
       "48  0.644638     No Downsample Cleaned CountV T (1, 2) None LR  2.395410e-01\n",
       "60  0.641428     No Downsample Cleaned CountV F (1, 2) None LR  2.359062e-01\n",
       "84  0.640102      No Downsample Cleaned TFIDV F (1, 2) None LR  2.595899e-01\n",
       "72  0.638673      No Downsample Cleaned TFIDV T (1, 2) None LR  2.855823e-01\n",
       "53  0.633056  No Downsample Cleaned CountV T (1, 3) english LR  1.777751e-01\n",
       "49  0.632324  No Downsample Cleaned CountV T (1, 2) english LR  2.184303e-01\n",
       "88  0.632157      No Downsample Cleaned TFIDV F (1, 3) None LR  9.998686e-01\n",
       "65  0.632003  No Downsample Cleaned CountV F (1, 3) english LR  2.192545e-01\n",
       "57  0.631847  No Downsample Cleaned CountV T (1, 4) english LR  1.596882e-01\n",
       "76  0.630763      No Downsample Cleaned TFIDV T (1, 3) None LR  9.998404e-01\n",
       "69  0.630649  No Downsample Cleaned CountV F (1, 4) english LR  1.933252e-01\n",
       "61  0.628897  No Downsample Cleaned CountV F (1, 2) english LR  1.985770e-01\n",
       "92  0.623099      No Downsample Cleaned TFIDV F (1, 4) None LR  9.999696e-01\n",
       "80  0.618892      No Downsample Cleaned TFIDV T (1, 4) None LR  9.999574e-01\n",
       "4   0.617105        Downsample Cleaned CountV T (1, 3) None LR  8.821356e-01\n",
       "8   0.616211        Downsample Cleaned CountV T (1, 4) None LR  8.861428e-01\n",
       "16  0.615883        Downsample Cleaned CountV F (1, 3) None LR  8.630417e-01\n",
       "0   0.615796        Downsample Cleaned CountV T (1, 2) None LR  8.803717e-01\n",
       "20  0.615759        Downsample Cleaned CountV F (1, 4) None LR  8.866049e-01\n",
       "12  0.613679        Downsample Cleaned CountV F (1, 2) None LR  8.589295e-01\n",
       "85  0.610002   No Downsample Cleaned TFIDV F (1, 2) english LR  3.149623e-01\n",
       "73  0.608004   No Downsample Cleaned TFIDV T (1, 2) english LR  3.061602e-01\n",
       "1   0.602546     Downsample Cleaned CountV T (1, 2) english LR  8.263129e-01\n",
       "13  0.601552     Downsample Cleaned CountV F (1, 2) english LR  8.251703e-01\n",
       "5   0.601429     Downsample Cleaned CountV T (1, 3) english LR  8.405311e-01\n",
       "17  0.600690     Downsample Cleaned CountV F (1, 3) english LR  8.150905e-01\n",
       "..       ...                                               ...           ...\n",
       "10  0.540632        Downsample Cleaned CountV T (1, 4) None NB  8.400820e-01\n",
       "22  0.540632        Downsample Cleaned CountV F (1, 4) None NB  8.400820e-01\n",
       "46  0.540632         Downsample Cleaned TFIDV F (1, 4) None NB  8.400820e-01\n",
       "29  0.540184      Downsample Cleaned TFIDV T (1, 3) english LR  9.999998e-01\n",
       "45  0.537733      Downsample Cleaned TFIDV F (1, 4) english LR  9.999999e-01\n",
       "33  0.531954      Downsample Cleaned TFIDV T (1, 4) english LR  1.000000e+00\n",
       "63  0.530934  No Downsample Cleaned CountV F (1, 2) english NB  2.652246e-13\n",
       "75  0.530934   No Downsample Cleaned TFIDV T (1, 2) english NB  2.652246e-13\n",
       "87  0.530934   No Downsample Cleaned TFIDV F (1, 2) english NB  2.652246e-13\n",
       "51  0.530934  No Downsample Cleaned CountV T (1, 2) english NB  2.652246e-13\n",
       "67  0.521683  No Downsample Cleaned CountV F (1, 3) english NB  3.098366e-38\n",
       "55  0.521683  No Downsample Cleaned CountV T (1, 3) english NB  3.098366e-38\n",
       "79  0.521683   No Downsample Cleaned TFIDV T (1, 3) english NB  3.098366e-38\n",
       "91  0.521683   No Downsample Cleaned TFIDV F (1, 3) english NB  3.098366e-38\n",
       "50  0.517715     No Downsample Cleaned CountV T (1, 2) None NB  6.819512e-10\n",
       "62  0.517715     No Downsample Cleaned CountV F (1, 2) None NB  6.819512e-10\n",
       "74  0.517715      No Downsample Cleaned TFIDV T (1, 2) None NB  6.819512e-10\n",
       "86  0.517715      No Downsample Cleaned TFIDV F (1, 2) None NB  6.819512e-10\n",
       "59  0.513580  No Downsample Cleaned CountV T (1, 4) english NB  2.362725e-60\n",
       "83  0.513580   No Downsample Cleaned TFIDV T (1, 4) english NB  2.362725e-60\n",
       "71  0.513580  No Downsample Cleaned CountV F (1, 4) english NB  2.362725e-60\n",
       "95  0.513580   No Downsample Cleaned TFIDV F (1, 4) english NB  2.362725e-60\n",
       "78  0.494081      No Downsample Cleaned TFIDV T (1, 3) None NB  2.580253e-43\n",
       "90  0.494081      No Downsample Cleaned TFIDV F (1, 3) None NB  2.580253e-43\n",
       "66  0.494081     No Downsample Cleaned CountV F (1, 3) None NB  2.580253e-43\n",
       "54  0.494081     No Downsample Cleaned CountV T (1, 3) None NB  2.580253e-43\n",
       "82  0.479763      No Downsample Cleaned TFIDV T (1, 4) None NB  2.058903e-91\n",
       "70  0.479763     No Downsample Cleaned CountV F (1, 4) None NB  2.058903e-91\n",
       "58  0.479763     No Downsample Cleaned CountV T (1, 4) None NB  2.058903e-91\n",
       "94  0.479763      No Downsample Cleaned TFIDV F (1, 4) None NB  2.058903e-91\n",
       "\n",
       "[96 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create models\n",
    "dataset = [data_downsample, data_nodownsample]\n",
    "vectorizer_type = [CountVectorizer, TfidfVectorizer]\n",
    "binary_type = [True, False]\n",
    "ngram = [(1,2), (1,3), (1,4)]\n",
    "stop_word = [None, stop]\n",
    "model_type = [LogisticRegression(), BernoulliNB()]\n",
    "\n",
    "model_initialize = []\n",
    "for h in dataset:\n",
    "    for i in vectorizer_type:\n",
    "        for j in binary_type:\n",
    "            for k in ngram:\n",
    "                for l in model_type:\n",
    "                    for m in stop_word:\n",
    "                        model_initialize.append(model_vectorize(h, i, j, k, m, l))\n",
    "                    \n",
    "#create labels\n",
    "dataset_label = ['Downsample Cleaned', 'No Downsample Cleaned']\n",
    "vectorizer_type_label = ['CountV', 'TFIDV']\n",
    "binary_type_label = ['T', 'F']\n",
    "ngram_label = [(1,2), (1,3), (1,4)]\n",
    "stop_word_label = ['None', 'english']\n",
    "model_type_label = ['LR', 'NB']\n",
    "\n",
    "labels = []\n",
    "for h in dataset_label:    \n",
    "    for i in vectorizer_type_label:\n",
    "        for j in binary_type_label:\n",
    "            for k in ngram_label:\n",
    "                for l in model_type_label:\n",
    "                    for m in stop_word_label:\n",
    "                        label = '%s %s %s %s %s %s' %(h, i, j, k, m, l)\n",
    "                        labels.append(label)\n",
    "                        \n",
    "fscores = []\n",
    "thresholds = []\n",
    "for i in model_initialize:\n",
    "    fscores.append(i[0])\n",
    "    thresholds.append(i[1])\n",
    "metrics = pd.DataFrame({'label': labels,'fscore': fscores, 'threshold': thresholds})\n",
    "metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bestmodel = train_data.copy()\n",
    "train_bestmodel['question_text'] = train_bestmodel[\"question_text\"].apply(lambda x: clean(x))\n",
    "test_bestmodel = test_data.copy()\n",
    "test_bestmodel['question_text'] = test_bestmodel[\"question_text\"].apply(lambda x: clean(x))\n",
    "\n",
    "test_results = run_model(train_bestmodel, test_bestmodel, CountVectorizer, True, (1,4), None, LogisticRegression(), .1636935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame()\n",
    "test_results['index'] = test_data.index\n",
    "test_results['prediction_percent'] = test[0]\n",
    "test_results['prediction'] = test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = test_results.drop('prediction_percent', 1)\n",
    "results.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "ex = []\n",
    "for text in train_data.question_text:\n",
    "    if 'gen x' in text:\n",
    "        count = count +1\n",
    "        ex.append(text)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what issu can millenni and gen x agre upon',\n",
       " 'whi do gen x peopl react more with insensit toward their mentallyil children',\n",
       " 'a gaseous hydrogen x was burnt in excess of oxygen a #### dmÂ³ sampl of x at stp game ###g of co# how mani catom are there in one molecul of x',\n",
       " 'what is like for a gen x date a millenni',\n",
       " 'what is the composit of gen x and gen y in the popul of bangalor',\n",
       " 'what is the biggest mistak the indian gen x did',\n",
       " 'how can i switch from be gen x to gen y',\n",
       " 'what is the volkswagen xl# car about',\n",
       " 'whi are indian men gen x and earlier so egoist']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
